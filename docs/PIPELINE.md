# ğŸ”„ Complete Pipeline Workflow

This document explains the entire workflow of the Multimodal RAG Chatbot, from document processing through response display.

---

## ğŸ“‹ Table of Contents

1. [Document Processing Pipeline](#document-processing-pipeline)
2. [Query Processing Pipeline](#query-processing-pipeline)
3. [Final Query Construction](#final-query-construction)
4. [Response Generation](#response-generation)
5. [Response Display](#response-display)
6. [Complete End-to-End Flow](#complete-end-to-end-flow)

---

## Document Processing Pipeline

### Overview

When a user uploads documents (PDFs, images, or text files), the system processes them:

```
User Uploads Files
    â”‚
    â”œâ”€ PDF File â†’ Extract text, images, tables
    â”œâ”€ Image File â†’ Load and encode to base64 + OCR
    â””â”€ Text File â†’ Read and chunk text
    â”‚
    â–¼
All chunks with metadata
    â”‚
    â–¼
CLIP Embedding (Unified vector space)
    â”‚
    â”œâ”€ Text chunks â†’ CLIP Text Encoder â†’ 512-dim
    â”œâ”€ Image chunks â†’ CLIP Image Encoder â†’ 512-dim
    â””â”€ Table chunks â†’ CLIP Text Encoder â†’ 512-dim
    â”‚
    â–¼
Store in FAISS Index + BM25 Index
```

### PDF Processing Steps

1. **Text Extraction** (PyMuPDF)
   - Extract text from each page
   - Split into chunks (1000 chars, 200 overlap)
   - Create Chunk objects with metadata

2. **Image Extraction** (PyMuPDF)
   - Extract image bytes
   - Convert to PIL Image
   - Encode to Base64 (for storage)
   - Run OCR (EasyOCR) for description
   - Create image Chunk with OCR text + base64 data

3. **Table Extraction** (PyMuPDF)
   - Find tables in page
   - Convert to Markdown format
   - Create table Chunk with Markdown content

---

## Query Processing Pipeline

### Overview

```
User Query
    â”‚
    â–¼
1. Reformulation (Check conversation history)
    â”‚
    â–¼
2. Embedding (CLIP Text Encoder)
    â”‚
    â–¼
3. Hybrid Retrieval (FAISS + BM25 + RRF)
    â”‚
    â–¼
4. Reranking (Cross-Encoder)
    â”‚
    â–¼
5. Build Context (Top results)
    â”‚
    â–¼
6. Construct Final Prompt
    â”‚
    â–¼
Send to Gemini API
```

### Step-by-Step Details

#### Step 1: Query Reformulation
```
Check conversation memory:
- If follow-up question â†’ Reformulate using LLM to make standalone
- If new question â†’ Use as-is

Example:
User: "Tell me more about the chart"
Memory: "Previous: What are the main findings?"
Result: "Tell me more about the chart from the main findings mentioned in the report"
```

#### Step 2: Query Embedding
```
Reformulated query â†’ CLIP Text Encoder â†’ 512-dim vector
Example: [0.12, -0.45, 0.89, ..., 0.33]
```

#### Step 3: Hybrid Retrieval
```
A) FAISS Semantic Search (60% weight)
   Query vector â†’ Search FAISS â†’ Top 10 semantic matches
   
B) BM25 Keyword Search (40% weight)
   Query text â†’ Search BM25 â†’ Top 10 keyword matches
   
C) Reciprocal Rank Fusion (RRF)
   Combine scores â†’ Final ranking â†’ Top 10 candidates
```

#### Step 4: Reranking
```
For each of 10 candidates:
   Cross-Encoder(query, chunk) â†’ Relevance score
   
Sort by score â†’ Keep top 3 results
```

---

## Final Query Construction

### What Gets Sent to Gemini API

```
FINAL PROMPT = System Prompt + Context + History + Query

System Prompt:
"You are a helpful AI assistant that answers questions based on provided context.
Always cite your sources using [Source X] format. Be accurate and concise."

+

Context (from top 3 reranked results):
"[Source 1: report.pdf, Page 1]
Main findings include a 25% increase in sales compared to last quarter...

[Source 2: report.pdf, Page 3]
The bar chart shows market trends with strong growth in Q4...

[Source 3: report.pdf, Page 5]
Key recommendations: Expand team, increase marketing budget..."

+

Conversation History (last 2 turns from memory):
"User: What are the main findings?
Assistant: The report shows three main findings: 1) 25% sales increase, 2) Market growth, 3) Key recommendations.

User: Tell me more about the chart."

+

User Query (reformulated):
"Tell me more about the chart showing sales data from the report"
```

This complete prompt is sent to Gemini API with streaming enabled.

---

## Response Generation

### How Gemini Responds

```
Gemini API receives complete prompt
    â”‚
    â–¼
Process with LLM (gemini-2.0-flash)
    â”‚
    â”œâ”€ Read context and understand query
    â”œâ”€ Generate response
    â”œâ”€ Include source citations
    â””â”€ Stream tokens back
    â”‚
    â–¼
Response Stream (token by token)
"The" â†’ "chart" â†’ "shows" â†’ "market" â†’ "trends"...
```

### Response Structure

```
Response from Gemini:
"The chart shows market trends with strong growth in Q4. Based on the data 
visualization, we can see:

1. **Q4 Growth**: Sales increased significantly
2. **Trend Analysis**: Upward trajectory suggests sustainable growth
3. **Market Position**: Position strengthened compared to competitors

[Source 1: report.pdf, Page 3]
[Source 2: report.pdf, Page 1]
[Source 3: report.pdf, Page 5]"
```

---

## Response Display

### Streamlit UI Display

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STREAMLIT CHAT INTERFACE          â”‚
â”‚                                        â”‚
â”‚ User:                                  â”‚
â”‚ "Tell me more about the chart..."     â”‚
â”‚                                        â”‚
â”‚ Assistant (Streaming):                 â”‚
â”‚ "The chart shows market trends with   â”‚
â”‚  strong growth in Q4..."               â”‚
â”‚                                        â”‚
â”‚ ğŸ“š View Sources (Expandable)          â”‚
â”‚   Source 1: report.pdf, Page 3        â”‚
â”‚   "The bar chart shows..."             â”‚
â”‚                                        â”‚
â”‚   Source 2: report.pdf, Page 1        â”‚
â”‚   "Main findings include..."           â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Flow

```python
# Display user message in chat
with st.chat_message("user"):
    st.write(user_query)

# Display assistant response (streaming)
with st.chat_message("assistant"):
    response_placeholder = st.empty()
    full_response = ""
    
    for chunk in response:  # Stream from Gemini
        if chunk.text:
            full_response += chunk.text
            response_placeholder.markdown(full_response)  # Real-time update
    
# Display source citations
with st.expander("ğŸ“š View Sources"):
    for i, source in enumerate(retrieved_sources, 1):
        st.write(f"**Source {i}: {source.source_file}, Page {source.page_number}**")
        st.write(source.content[:300] + "...")

# Save to conversation memory for next turn
conversation_memory.add_user_message(user_query)
conversation_memory.add_assistant_message(full_response)
```

---

## Complete End-to-End Flow

### Full Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DOCUMENT UPLOAD & PROCESSING                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: "report.pdf"                                        â”‚
â”‚ Processing:                                                â”‚
â”‚ â”œâ”€ Extract 200 text chunks                                â”‚
â”‚ â”œâ”€ Extract 5 images with base64 + OCR                     â”‚
â”‚ â”œâ”€ Extract 3 tables as markdown                           â”‚
â”‚ â””â”€ Total: 208 Chunk objects                               â”‚
â”‚ Embedding:                                                 â”‚
â”‚ â”œâ”€ Text â†’ CLIP Text â†’ 512-dim vectors                     â”‚
â”‚ â”œâ”€ Images â†’ CLIP Image â†’ 512-dim vectors                  â”‚
â”‚ â””â”€ Tables â†’ CLIP Text â†’ 512-dim vectors                   â”‚
â”‚ Storage:                                                   â”‚
â”‚ â”œâ”€ FAISS Index: 208 vectors                               â”‚
â”‚ â””â”€ BM25 Index: 208 documents                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. USER QUERIES                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Query: "What are the main findings?"                       â”‚
â”‚                                                            â”‚
â”‚ Processing:                                               â”‚
â”‚ â”œâ”€ No previous context â†’ Use query as-is                  â”‚
â”‚ â”œâ”€ Embed query â†’ 512-dim vector                           â”‚
â”‚ â”œâ”€ FAISS search (k=10) + BM25 search (k=10)              â”‚
â”‚ â”œâ”€ RRF fusion â†’ Top 10 candidates                         â”‚
â”‚ â”œâ”€ Cross-Encoder reranking â†’ Top 3 results               â”‚
â”‚ â””â”€ Selected chunks ready for LLM                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. PROMPT CONSTRUCTION                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Final Prompt:                                              â”‚
â”‚ â”œâ”€ System: "You are a helpful assistant..."              â”‚
â”‚ â”œâ”€ Context: [Source 1], [Source 2], [Source 3]           â”‚
â”‚ â”œâ”€ History: (empty for first query)                       â”‚
â”‚ â””â”€ Query: "What are the main findings?"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. GEMINI API RESPONSE (STREAMING)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Token stream:                                              â”‚
â”‚ "The" "report" "shows" "three" "main" "findings"...       â”‚
â”‚                                                            â”‚
â”‚ Full Response:                                             â”‚
â”‚ "The report shows three main findings:                    â”‚
â”‚  1. Sales increased 25%                                   â”‚
â”‚  2. Market growth evident                                 â”‚
â”‚  3. Key recommendations                                   â”‚
â”‚  [Source 1], [Source 2], [Source 3]"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. DISPLAY IN STREAMLIT                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ User message displayed                                 â”‚
â”‚ â”œâ”€ Assistant response displayed (real-time streaming)     â”‚
â”‚ â”œâ”€ Source citations visible                               â”‚
â”‚ â”œâ”€ "View Sources" expandable section                      â”‚
â”‚ â””â”€ Response saved to memory                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. MEMORY UPDATE FOR NEXT TURN                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationMemory:                                        â”‚
â”‚ â”œâ”€ Turn 1:                                                â”‚
â”‚ â”‚  User: "What are the main findings?"                    â”‚
â”‚ â”‚  Assistant: "The report shows three main findings..."   â”‚
â”‚ â”‚                                                         â”‚
â”‚ â””â”€ Ready for follow-up questions                          â”‚
â”‚    (Next query will use this history for reformulation)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Components Map

| Component | File | Function | Input â†’ Output |
|-----------|------|----------|-----------------|
| Document Loading | `document_processor.py` | `load_document()` | PDF/Image/Text â†’ List[Chunk] |
| Text Extraction | `document_processor.py` | `process_pdf()` | PDF â†’ Text chunks |
| Image Processing | `document_processor.py` | `process_image()` | Image â†’ Chunk with base64 |
| OCR | `document_processor.py` | `run_ocr()` | PIL Image â†’ Text string |
| Embedding | `embedding_engine.py` | `embed_text()`, `embed_image()` | Text/Image â†’ 512-dim vector |
| FAISS Indexing | `embedding_engine.py` | `FAISSIndex` | Vectors â†’ Index |
| BM25 Indexing | `retrieval_engine.py` | `BM25Index` | Text â†’ Index |
| Query Reformulation | `llm_service.py` | `reformulate_query()` | Query + Memory â†’ Standalone query |
| Hybrid Search | `retrieval_engine.py` | `HybridRetriever` | Query â†’ Top 10 candidates |
| Reranking | `retrieval_engine.py` | `Reranker` | Query + Candidates â†’ Top 3 |
| Prompt Building | `rag_pipeline.py` | `query()` | Context + Query â†’ Final prompt |
| LLM Response | Google Gemini API | N/A | Prompt â†’ Streaming response |
| Display | `components/chat_interface.py` | Streamlit widgets | Response â†’ Chat UI |
| Memory | `llm_service.py` | `ConversationMemory` | Query + Response â†’ Updated memory |

---

## Data Flow Diagram

```
User Input
    â”‚
    â”œâ”€â†’ Document Upload?
    â”‚   â””â”€â†’ document_processor.py
    â”‚       â””â”€â†’ Extract + Chunk â†’ embedding_engine.py
    â”‚           â””â”€â†’ FAISS + BM25 Indices
    â”‚
    â””â”€â†’ Query Input
        â”‚
        â”œâ”€â†’ llm_service.py (Reformulation)
        â”‚   â””â”€â†’ ConversationMemory
        â”‚
        â”œâ”€â†’ embedding_engine.py (Embed)
        â”‚   â””â”€â†’ 512-dim query vector
        â”‚
        â”œâ”€â†’ retrieval_engine.py (Search)
        â”‚   â”œâ”€â†’ FAISS (semantic)
        â”‚   â”œâ”€â†’ BM25 (keyword)
        â”‚   â””â”€â†’ RRF Fusion â†’ Top 10
        â”‚
        â”œâ”€â†’ retrieval_engine.py (Rerank)
        â”‚   â””â”€â†’ Cross-Encoder â†’ Top 3
        â”‚
        â”œâ”€â†’ rag_pipeline.py (Prompt)
        â”‚   â””â”€â†’ Final prompt construction
        â”‚
        â”œâ”€â†’ Google Gemini API
        â”‚   â””â”€â†’ Streaming response
        â”‚
        â”œâ”€â†’ llm_service.py (Memory)
        â”‚   â””â”€â†’ Save to ConversationMemory
        â”‚
        â””â”€â†’ Streamlit (Display)
            â”œâ”€â†’ User message
            â”œâ”€â†’ Assistant response (streaming)
            â”œâ”€â†’ Source citations
            â””â”€â†’ Expandable sources
```

---

## Performance Characteristics

- **Document Processing**: ~100 pages/minute
- **Query to Response**: 2-5 seconds (including API latency)
- **Memory Usage**: 2GB baseline + 1GB per 1000 chunks
- **Vector Similarity**: Cosine (L2 normalized in FAISS)
- **Embedding Dimension**: 512-dim (CLIP ViT-B-32)
- **Max Conversation Turns**: 5 (in memory)
- **Retrieved Context Size**: Top 3 chunks
- **Reranking Speed**: ~50ms per query

---

**Document Version**: 1.0  
**Created**: December 20, 2025
